# Attention Mechanisms

## seq2seq Recap
![seq2seq](../assets2/seq2seq.png)
The encoder is confined to sending a single vector(context vector, between encoder and decoder.), no matter how long or short the input sequence is.

Choosing a reasonable size of this vector makes the model have problems with long input sequence. It has problems remembering information that was there is the early sequence and it doesn't have enough information to capture all of the information from that long input sequence.

What if just use a very large number of hidden units in the encoder so that the context is very large, but then your model **overfits** with short sequences, and you take a performance hit as you increase the number of parameters.

This is the problem that attention soloves.

## Encoding -- Attention Overview
![Encoder Attention](../assets2/encoder_attention.png)
First, the Encoder process the input sequence just like the model without attention, one word at a time, producing a hidden state, and using that hidden state in the next step.

Next, the model passws a Context Vector to the decoder. But unlike the contecxt vector in the model without attention, this one is **not just the final hidden state, it's all of the hidden states**.

This gives us the benefit of having the flexibility in the context size, so longer sequence can have longer context vector that better capture the information from the input sequence.

One additional point that's important for the intuition of attention is that each hidden state is associated the most with the part of the input sequence that preceded how where word was generated.

So, the first hidden state(h1) was outputted after processing the first word, so it captures the essence of the first word the most. And so, when we focus on this vector, we will be focusing on that word the most, the same with the second hidden state with the second word, in the third with the third word. Even though that class and third vector incorporates a little bit of everything that preceded it as well.

## Decoding -- Attention Overview
At every time step, an attention decoder pays attention to the appropriate part of the input sqeuence using the context vector.

How does the attention decoder know which of the parts of the input sequence to focus on at each step? That process is learned during the traning phase. And it's not just stupidly going sequentially from the first and the second to the third, it can learn some sophisticated behavior.
![decode attention example](../assets2/decode_attention_example.png)

In models without attention, we'd only feed the last context vector to the decoder RNN in addition to the embedding of the end token(e.g. '<END>'), and it will begin ito generate an element of the output sequence at each timestep. The case is different in an attention decoder, however.

An attention decoder has the ability to look at the inputted words and the decoders own hidden state, and then it would do the following:
* It would use a `scoring function` to score each hidden state in the context matrix. (talk later about the scoring function). 

* After scoring, each context vector would end up with a certain score, and if we feed these scores into a softmax function, we end up with scores that are all positive, between 0 and 1 and all sum up to one. These values are how much each vector will be expressed in the `attention vector` that the decoder will look at before producing and output.

* Simply multiplying each vector by its softmax score and then summing up these vectors produces an `attention context vector`. This is a basic weighted sum operation. The context vector is an important milestone in this process, but it's not the end goal.

* merge context vector with a decoders hidden state to create the real output of the decoder at the timestep.//TODO
![Attention Decoding](../assets2/attention_decoding.png)

The decoder has now looked at the input word and add the attention context vector, which focuses its attention on the appropriate place in the input sequence. And so, it produces a hidden state and it produces the first word in the output sequence.

In the next timestep, the RNN takes its previous output as an input and it generates its own context vector for that timestep as well as the hidden state from the previous timestep. And that produces a new hidden state for the decoder and a new word in the output sequence. And this goes on until we've completed our output sequence.
![Attention Decoding 2](../assets2/attention_decoding2.png)
